Это файл для вопросов, ответов и комментариев между студентом и ревьювером

 - Вместо использования метода write можно использовать метод jdbc с опцией batchsize, чтобы улучшить производительность записи в базу данных. Это позволит отправлять данные в PostgreSQL пакетами, вместо записи каждой строки отдельно.
Ответ: Окей, сделал с ним, надеюсь что написал правильно ибо полной java-doc подобной документации с примерами этого для метода не нашлось.

 - Во write_kafka_stream, удалить параметр schema, так как структура DataFrame уже определена. И лучше добавить привязку к переменной df, чтобы использовать ее непосредственно
Ответ: Да, я удалил в вызове, а в описании ф-ции забыл - удалил.

 - Используй настройку startingOffsets для чтения только новых данных из Kafka, чтобы не повторять уже обработанные сообщения. Мы можем хранить смещение последнего считанного сообщения и указывать его как начальное смещение при чтении нового батча.
Ответ: Окей, тут разобрался, дописал - спасибо за подсказку. Этот аспект вообще не очень понятен в этом спринте: вроде-бы Spark Streaming из коробки должен читать olny-once, но как это обеспечивается непонятно. С другой стороны есть много опций в конфиге чтения стрима, которые в т.ч. гарантируют only-once. В общем, потом буду отдельно это изучать.

 - Перед чтением из таблицы PostgreSQL можно добавить проверку наличия таблицы, чтобы избежать ошибок, если таблицы не существует.
Ответ: Да, это полезно, чтобы не было так: SparkSession началась, чтение kafka стрима началось, а таблицы нет. Немного костыльно, но сделал.
